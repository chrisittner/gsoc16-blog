<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Structure Learning for pgmpy</title><link href="http://pgmpy.chrisittner.de/" rel="alternate"></link><link href="http://pgmpy.chrisittner.de/feed.rss" rel="self"></link><id>http://pgmpy.chrisittner.de/</id><updated>2016-06-24T00:00:00+02:00</updated><entry><title>Score-based Structure Learning BNs</title><link href="http://pgmpy.chrisittner.de/2016/06/24/score-based-structure-learning-bns.html" rel="alternate"></link><published>2016-06-24T00:00:00+02:00</published><author><name>Chris Ittner</name></author><id>tag:pgmpy.chrisittner.de,2016-06-24:2016/06/24/score-based-structure-learning-bns.html</id><summary type="html">&lt;p&gt;With a bit of delay, I am now working on a basic PR for score-based structure estimation for Bayesian Networks.
It comes with two ingredients:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;StructureScore&lt;/code&gt;-Class and its subclasses &lt;code&gt;BayesianScore&lt;/code&gt; and &lt;code&gt;BICScore&lt;/code&gt;. They are initialized with a data set and provide a &lt;code&gt;score&lt;/code&gt;-method to compute how well a given &lt;code&gt;BayesianModel&lt;/code&gt; can be fitted to the data, according to different criteria. Since those scores are decomposable for BNs, a &lt;code&gt;local_score&lt;/code&gt;-method is also exposed for node-by-node computation.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;StructureSearch&lt;/code&gt;-Class and its subclasses &lt;code&gt;ExhaustiveSearch&lt;/code&gt; and &lt;code&gt;GradientDescentSearch&lt;/code&gt;. They are initialized with a &lt;code&gt;StructureScore&lt;/code&gt;-instance and optimize that score over all &lt;code&gt;BayesianModel&lt;/code&gt;s. The latter subclass has a number of optional search enhancements.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So far &lt;code&gt;BayesianScore&lt;/code&gt; supports BDeu and K2 priors, I’ll think for a good interface to specify other prior weights.
With K2 priors the score is given by the following form:&lt;/p&gt;
&lt;div class="math"&gt;$$score^{K2}_D(m) = \log(P(m)) + \sum_{X\in nodes(m)} local\_score^{K2}_D(X, parents_m(X))$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(P(m)\)&lt;/span&gt; is an optional structure prior that is quite negligible in practice. &lt;span class="math"&gt;\(local\_score^{K2}\)&lt;/span&gt; is computed for each node as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$local\_score^{K2}_D(X, P_X) = \sum_{j=1}^{q(P_X)} (\log(\frac{(r-1)!}{(N_j+r-1)!}) + \sum_{k=1}^r \log(N_{jk}!))$$&lt;/div&gt;
&lt;p&gt;Where &lt;span class="math"&gt;\(r\)&lt;/span&gt; is the cardinality of the variable &lt;span class="math"&gt;\(X\)&lt;/span&gt;, &lt;span class="math"&gt;\(q(P_X)\)&lt;/span&gt; is the product of the cardinalities of the parents of &lt;span class="math"&gt;\(X\)&lt;/span&gt; (= the possible states of &lt;span class="math"&gt;\(P_X\)&lt;/span&gt;) and &lt;span class="math"&gt;\(N_{jk}\)&lt;/span&gt; is the number of times that variable &lt;span class="math"&gt;\(X\)&lt;/span&gt; is in state &lt;span class="math"&gt;\(k\)&lt;/span&gt; while parents are in state &lt;span class="math"&gt;\(j\)&lt;/span&gt; in the data sample. Finally, &lt;span class="math"&gt;\(N_j:=\sum_{k=1}^r N_{jk}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;PR will follow shortly.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary></entry><entry><title>Bayesian Parameter Estimation for BNs</title><link href="http://pgmpy.chrisittner.de/2016/06/10/bayesian-parameter-estimation-for-bns.html" rel="alternate"></link><published>2016-06-10T00:00:00+02:00</published><author><name>Chris Ittner</name></author><id>tag:pgmpy.chrisittner.de,2016-06-10:2016/06/10/bayesian-parameter-estimation-for-bns.html</id><summary type="html">&lt;p&gt;Now that ML Parameter Estimation works well, I’ve turned to Bayesian Parameter Estimation (all for discrete variables).&lt;/p&gt;
&lt;p&gt;The Bayesian approach is, in practice, very similar to the ML case.
Both involves counting how often each state of the variable obtains in the data, conditional of the parents state.
I thus factored out &lt;code&gt;state_count&lt;/code&gt;-method and put it the &lt;code&gt;estimators.BaseEstimator&lt;/code&gt; class from which all estimators inherit.
MLE is basically done after taking and normalizing the state counts.
For Bayesian Estimation (with dirichlet priors), one additionally specifies so-called &lt;code&gt;pseudo_counts&lt;/code&gt; for each variable state that encode prior beliefs. The state counts are then added to these virtual counts before normalization. My next PR will implement &lt;code&gt;estimators.BayesianEstimator&lt;/code&gt; to compute the CPD parameters for a &lt;code&gt;BayesianModel&lt;/code&gt;, given data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: &lt;a href="https://github.com/pgmpy/pgmpy/pull/696"&gt;#696&lt;/a&gt; has been merged and also added basic support for missing data.&lt;/p&gt;
&lt;p&gt;```python&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;import pandas as pd
from pgmpy.models import BayesianModel
from pgmpy.estimators import BayesianEstimator
data = pd.DataFrame(data={‘A’: [0, 0, 1], ‘B’: [0, 1, 0], ‘C’: [1, 1, 0]})
model = BayesianModel([(‘A’, ‘C’), (‘B’, ‘C’)])
estimator = BayesianEstimator(model, data)
print(estimator.estimate_cpd(‘A’, prior_type=”dirichlet”, pseudo_counts=[1, 1]))
╒══════╤═════╕
│ A(0) │ 0.6 │
├──────┼─────┤
│ A(1) │ 0.4 │
╘══════╧═════╛
print(estimator.estimate_cpd(‘A’, prior_type=”dirichlet”, pseudo_counts=[2, 2]))
╒══════╤══════════╕
│ A(0) │ 0.571429 │
├──────┼──────────┤
│ A(1) │ 0.428571 │
╘══════╧══════════╛
print(estimator.estimate_cpd(‘A’, prior_type=”dirichlet”, pseudo_counts=[5, 5]))
╒══════╤══════════╕
│ A(0) │ 0.538462 │
├──────┼──────────┤
│ A(1) │ 0.461538 │
╘══════╧══════════╛
for cpd in estimator.get_parameters(prior_type=”BDeu”, equivalent_sample_size=10):
…     print(cpd)
…
╒══════╤══════════╕
│ A(0) │ 0.538462 │
├──────┼──────────┤
│ A(1) │ 0.461538 │
╘══════╧══════════╛
╒══════╤═════════════════════╤═════════════════════╤═════════════════════╤══════╕
│ A    │ A(0)                │ A(0)                │ A(1)                │ A(1) │
├──────┼─────────────────────┼─────────────────────┼─────────────────────┼──────┤
│ B    │ B(0)                │ B(1)                │ B(0)                │ B(1) │
├──────┼─────────────────────┼─────────────────────┼─────────────────────┼──────┤
│ C(0) │ 0.35714285714285715 │ 0.35714285714285715 │ 0.6428571428571429  │ 0.5  │
├──────┼─────────────────────┼─────────────────────┼─────────────────────┼──────┤
│ C(1) │ 0.6428571428571429  │ 0.6428571428571429  │ 0.35714285714285715 │ 0.5  │
╘══════╧═════════════════════╧═════════════════════╧═════════════════════╧══════╛
╒══════╤══════════╕
│ B(0) │ 0.538462 │
├──────┼──────────┤
│ B(1) │ 0.461538 │
╘══════╧══════════╛&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;</summary></entry><entry><title>MLE Parameter Estimation for BNs</title><link href="http://pgmpy.chrisittner.de/2016/05/18/mle-parameter-estimation-for-bns.html" rel="alternate"></link><published>2016-05-18T00:00:00+02:00</published><author><name>Chris Ittner</name></author><id>tag:pgmpy.chrisittner.de,2016-05-18:2016/05/18/mle-parameter-estimation-for-bns.html</id><summary type="html">&lt;p&gt;At the moment pgmpy supports Maximum Likelihood Estimation (MLE) to estimate the conditional probability tables (CPTs) for the variables of a Bayesian Network, given some data set. In my first PR, I’ll refactor the current MLE parameter estimation code to make it a bit nicer to use.
This includes properly using pgmpy’s state name feature, removing the current limitation to &lt;code&gt;int&lt;/code&gt;-data and allowing to specify the states that each variable might take in advance, rather than reading it from the data. The latter will be necessary for Bayesian Parameter estimation, where non-occurring states get nonzero probabilities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; &lt;a href="https://github.com/pgmpy/pgmpy/pull/694"&gt;#694&lt;/a&gt; has been merged, and &lt;code&gt;MaximumLikelihoodEstimator&lt;/code&gt; now supports the above features, including non-numeric variables:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pgmpy.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BayesianModel&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pgmpy.estimators&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MaximumLikelihoodEstimator&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BayesianModel&lt;/span&gt;&lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="s1"&gt;'Light?'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Color'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Fruit'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Color'&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'Fruit'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Apple'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Apple'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Apple'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Banana'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Banana'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                          &lt;span class="s1"&gt;'Light?'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                          &lt;span class="s1"&gt;'Color'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'red'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="s1"&gt;'green'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'black'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'black'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="s1"&gt;'yellow'&lt;/span&gt;&lt;span class="p"&gt;]})&lt;/span&gt;
&lt;span class="n"&gt;mle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MaximumLikelihoodEstimator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_estimate_cpd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Color'&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;╒═══════════════╤═══════════════╤══════════════╤═══════════════╤═══════════════╕
│ Fruit         │ Fruit(Apple)  │ Fruit(Apple) │ Fruit(Banana) │ Fruit(Banana) │
├───────────────┼───────────────┼──────────────┼───────────────┼───────────────┤
│ Light?        │ Light?(False) │ Light?(True) │ Light?(False) │ Light?(True)  │
├───────────────┼───────────────┼──────────────┼───────────────┼───────────────┤
│ Color(black)  │ 1.0           │ 0.0          │ 1.0           │ 0.0           │
├───────────────┼───────────────┼──────────────┼───────────────┼───────────────┤
│ Color(green)  │ 0.0           │ 0.5          │ 0.0           │ 0.0           │
├───────────────┼───────────────┼──────────────┼───────────────┼───────────────┤
│ Color(red)    │ 0.0           │ 0.5          │ 0.0           │ 0.0           │
├───────────────┼───────────────┼──────────────┼───────────────┼───────────────┤
│ Color(yellow) │ 0.0           │ 0.0          │ 0.0           │ 1.0           │
╘═══════════════╧═══════════════╧══════════════╧═══════════════╧═══════════════╛
&lt;/pre&gt;&lt;/div&gt;</summary></entry><entry><title>GSoC proposal accepted!</title><link href="http://pgmpy.chrisittner.de/2016/04/26/first-post.html" rel="alternate"></link><published>2016-04-26T00:00:00+02:00</published><author><name>Chris Ittner</name></author><id>tag:pgmpy.chrisittner.de,2016-04-26:2016/04/26/first-post.html</id><summary type="html">&lt;p&gt;My proposal for &lt;a href="https://summerofcode.withgoogle.com/"&gt;Google Summer of Code 2016&lt;/a&gt; has 
been accepted :). This means that I will spend part of my summer working on the &lt;a href="http://pgmpy.org"&gt;pgmpy&lt;/a&gt; library. I will implement some techniques for Bayesian Network 
structure learning. You can have a look an my proposal 
&lt;a href="http://pgmpy.chrisittner.de/pages/gsoc-proposal.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As a first step, I set up this blog to document my progress. It is built with the &lt;a href="http://blog.getpelican.com/"&gt;Pelican&lt;/a&gt; static-site generator and hosted on &lt;a href="https://pages.github.com/"&gt;GitHub pages&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Updates will follow!&lt;/p&gt;</summary></entry></feed>