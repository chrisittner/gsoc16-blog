<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Structure Learning for pgmpy</title><link href="http://pgmpy.chrisittner.de/" rel="alternate"></link><link href="http://pgmpy.chrisittner.de/feed.rss" rel="self"></link><id>http://pgmpy.chrisittner.de/</id><updated>2016-07-23T00:00:00+02:00</updated><entry><title>Feature summary of BN structure learning in python pgm libraries</title><link href="http://pgmpy.chrisittner.de/2016/07/23/bn-structure-learning-feature-comparison.html" rel="alternate"></link><published>2016-07-23T00:00:00+02:00</published><author><name>Chris Ittner</name></author><id>tag:pgmpy.chrisittner.de,2016-07-23:2016/07/23/bn-structure-learning-feature-comparison.html</id><summary type="html">&lt;p&gt;This is a (possibly already outdated) summary of structure learning capabilities of existing Python libraries for general &lt;em&gt;Bayesian networks&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id="libpgmpgmlearner"&gt;&lt;a href="http://pythonhosted.org/libpgm/pgmlearner.html"&gt;libpgm.pgmlearner&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Discrete MLE Parameter estimation&lt;/li&gt;
&lt;li&gt;Discrete constraint-based Structure estimation&lt;/li&gt;
&lt;li&gt;Linear Gaussian MLE Parameter estimation&lt;/li&gt;
&lt;li&gt;Linear Gaussian constraint-based Structure estimation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Version 1.1, released 2012, Python 2&lt;/p&gt;
&lt;h3 id="bnfinder-also-here"&gt;&lt;a href="http://bioputer.mimuw.edu.pl/software/bnf/"&gt;bnfinder&lt;/a&gt; (also &lt;a href="https://github.com/sysbio-vo/bnfinder"&gt;here&lt;/a&gt;)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Discrete &amp;amp; Continuous score-based Structure estimation&lt;ul&gt;
&lt;li&gt;scores: MDL/BIC (default), BDeu, K2&lt;/li&gt;
&lt;li&gt;supports restriction to subset of data set, per node&lt;/li&gt;
&lt;li&gt;supports restrictions of parents set, per node&lt;/li&gt;
&lt;li&gt;allows to restrict the serach space (max number of parents)&lt;/li&gt;
&lt;li&gt;search method??&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Command line tool&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Version 2, 2011-2014?, Python 2&lt;/p&gt;
&lt;h3 id="pomegranate"&gt;&lt;a href="http://pomegranate.readthedocs.io/en/latest/bayesnet.html"&gt;pomegranate&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Discrete MLE Parameter estimation&lt;/li&gt;
&lt;li&gt;Can be used to estimate missing values in incomplete data sets prior to model parametrization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Version 0.4, 2016, Python 2, possibly Python 3&lt;/p&gt;
&lt;p&gt;Further relevant libraries include &lt;a href="https://github.com/pymc-devs/pymc3"&gt;PyMC&lt;/a&gt;,
&lt;a href="https://github.com/bayespy/bayespy"&gt;BayesPy&lt;/a&gt;, and the
&lt;a href="https://sourceforge.net/projects/pbnt.berlios/"&gt;Python Bayes Network Toolbox&lt;/a&gt;.
Also check out the &lt;a href="http://www.bnlearn.com/documentation/man/bnlearn-package.html"&gt;bnlearn&lt;/a&gt; R package for more functionality.&lt;/p&gt;</summary></entry><entry><title>Examples for basic BN learning,</title><link href="http://pgmpy.chrisittner.de/2016/07/07/examples-for-basic-bn-learning.html" rel="alternate"></link><published>2016-07-07T00:00:00+02:00</published><author><name>Chris Ittner</name></author><id>tag:pgmpy.chrisittner.de,2016-07-07:2016/07/07/examples-for-basic-bn-learning.html</id><summary type="html">&lt;p&gt;I’ll soon finish basic score-based structure estimation for &lt;code&gt;BayesianModel&lt;/code&gt;s. Below is the current state of my PR, with two examples.&lt;/p&gt;
&lt;h2 id="changes-in-pgmpyestimators"&gt;Changes in &lt;code&gt;pgmpy/estimators/&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;I rearranged the estimator classes to inherit from each other like this:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;.                                    MaximumLikelihoodEstimator
                                   /
                ParameterEstimator -- BayesianEstimator
              /
BaseEstimator                        ExhaustiveSearch
            | \                    /
            |   StructureEstimator -- HillClimbSearch
            |                      \
            |                        ConstraintBasedEstimator
            |
            |
            |                BayesianScore
            |              /
            StructureScore -- BicScore
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;BaseEstimator&lt;/code&gt; takes a data set and optionally &lt;code&gt;state_names&lt;/code&gt; and a flag for how to handle missing values. &lt;code&gt;ParameterEstimator&lt;/code&gt; and its subclasses additionally take a model. All &lt;code&gt;*Search&lt;/code&gt;-classes are initialized with a &lt;code&gt;StructureScore&lt;/code&gt;-instance (or by default &lt;code&gt;BayesianScore&lt;/code&gt;) in addition to the data set.&lt;/p&gt;
&lt;h2 id="example"&gt;Example&lt;/h2&gt;
&lt;p&gt;Given a data sets with &lt;code&gt;5&lt;/code&gt; or less variables, we can search through all &lt;code&gt;BayesianModels&lt;/code&gt; and find the best-scoring one, using &lt;code&gt;ExhaustiveSearch&lt;/code&gt; (currently 5 vars already takes a few minutes, but can be made faster):&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pgmpy.estimators&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ExhaustiveSearch&lt;/span&gt;

&lt;span class="c1"&gt;# create random data sample with 3 variables, where B and C are identical:&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'AB'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'C'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;est&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ExhaustiveSearch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;best_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;est&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;estimate&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;best_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nodes&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;best_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;all scores:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;est&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_scores&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The example first prints nodes and edges of the best-fitting model and then the scores for all possible  &lt;code&gt;BayesianModel&lt;/code&gt;s for this data set:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;['A','B','C']&lt;/span&gt;
&lt;span class="k"&gt;[('B','C')]&lt;/span&gt;

&lt;span class="err"&gt;all&lt;/span&gt; &lt;span class="err"&gt;scores:&lt;/span&gt;
&lt;span class="err"&gt;-24243.15030635083&lt;/span&gt; &lt;span class="k"&gt;[('A', 'C'), ('A', 'B')]&lt;/span&gt;
&lt;span class="err"&gt;-24243.149854387288&lt;/span&gt; &lt;span class="k"&gt;[('A', 'B'), ('C', 'A')]&lt;/span&gt;
&lt;span class="err"&gt;-24243.149854387288&lt;/span&gt; &lt;span class="k"&gt;[('A', 'C'), ('B', 'A')]&lt;/span&gt;
&lt;span class="err"&gt;-24211.96205284525&lt;/span&gt; &lt;span class="k"&gt;[('A', 'B')]&lt;/span&gt;
&lt;span class="err"&gt;-24211.96205284525&lt;/span&gt; &lt;span class="k"&gt;[('A', 'C')]&lt;/span&gt;
&lt;span class="err"&gt;-24211.961600881707&lt;/span&gt; &lt;span class="k"&gt;[('B', 'A')]&lt;/span&gt;
&lt;span class="err"&gt;-24211.961600881707&lt;/span&gt; &lt;span class="k"&gt;[('C', 'A')]&lt;/span&gt;
&lt;span class="err"&gt;-24211.961600881707&lt;/span&gt; &lt;span class="k"&gt;[('C', 'A'), ('B', 'A')]&lt;/span&gt;
&lt;span class="err"&gt;-24180.77379933967&lt;/span&gt; &lt;span class="k"&gt;[]&lt;/span&gt;
&lt;span class="err"&gt;-16603.134367431743&lt;/span&gt; &lt;span class="k"&gt;[('A', 'C'), ('A', 'B'), ('B', 'C')]&lt;/span&gt;
&lt;span class="err"&gt;-16603.13436743174&lt;/span&gt; &lt;span class="k"&gt;[('A', 'C'), ('A', 'B'), ('C', 'B')]&lt;/span&gt;
&lt;span class="err"&gt;-16603.133915468195&lt;/span&gt; &lt;span class="k"&gt;[('A', 'B'), ('C', 'A'), ('C', 'B')]&lt;/span&gt;
&lt;span class="err"&gt;-16603.133915468195&lt;/span&gt; &lt;span class="k"&gt;[('A', 'C'), ('B', 'A'), ('B', 'C')]&lt;/span&gt;
&lt;span class="err"&gt;-16571.946113926162&lt;/span&gt; &lt;span class="k"&gt;[('A', 'C'), ('B', 'C')]&lt;/span&gt;
&lt;span class="err"&gt;-16571.94611392616&lt;/span&gt; &lt;span class="k"&gt;[('A', 'B'), ('C', 'B')]&lt;/span&gt;
&lt;span class="err"&gt;-16274.052597732147&lt;/span&gt; &lt;span class="k"&gt;[('A', 'B'), ('B', 'C')]&lt;/span&gt;
&lt;span class="err"&gt;-16274.052597732145&lt;/span&gt; &lt;span class="k"&gt;[('A', 'C'), ('C', 'B')]&lt;/span&gt;
&lt;span class="err"&gt;-16274.0521457686&lt;/span&gt; &lt;span class="k"&gt;[('B', 'A'), ('B', 'C')]&lt;/span&gt;
&lt;span class="err"&gt;-16274.0521457686&lt;/span&gt; &lt;span class="k"&gt;[('C', 'A'), ('B', 'C')]&lt;/span&gt;
&lt;span class="err"&gt;-16274.0521457686&lt;/span&gt; &lt;span class="k"&gt;[('C', 'B'), ('B', 'A')]&lt;/span&gt;
&lt;span class="err"&gt;-16274.0521457686&lt;/span&gt; &lt;span class="k"&gt;[('C', 'A'), ('C', 'B')]&lt;/span&gt;
&lt;span class="err"&gt;-16274.0521457686&lt;/span&gt; &lt;span class="k"&gt;[('C', 'A'), ('B', 'A'), ('B', 'C')]&lt;/span&gt;
&lt;span class="err"&gt;-16274.0521457686&lt;/span&gt; &lt;span class="k"&gt;[('C', 'A'), ('C', 'B'), ('B', 'A')]&lt;/span&gt;
&lt;span class="err"&gt;-16242.864344226566&lt;/span&gt; &lt;span class="k"&gt;[('B', 'C')]&lt;/span&gt;
&lt;span class="err"&gt;-16242.864344226564&lt;/span&gt; &lt;span class="k"&gt;[('C', 'B')]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There is a big jump in score between those models where &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt; influence each other (~&lt;code&gt;-16274&lt;/code&gt;) and the rest (~&lt;code&gt;-24211&lt;/code&gt;), as expected since they are correlated.&lt;/p&gt;
&lt;h2 id="example-2"&gt;Example 2&lt;/h2&gt;
&lt;p&gt;I tried the same with the Kaggle titanic data set:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pgmpy.estimators&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ExhaustiveSearch&lt;/span&gt;

&lt;span class="c1"&gt;# data_link - "https://www.kaggle.com/c/titanic/download/train.csv"&lt;/span&gt;
&lt;span class="n"&gt;titanic_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'testdata/titanic_train.csv'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;titanic_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;titanic_data&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s2"&gt;"Survived"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Sex"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Pclass"&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="n"&gt;est&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ExhaustiveSearch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;titanic_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;est&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_scores&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-2072.9132364404695 []
-2069.071694164769 [('Pclass', 'Sex')]
-2069.0144197068785 [('Sex', 'Pclass')]
-2025.869489762676 [('Survived', 'Pclass')]
-2025.8559302273054 [('Pclass', 'Survived')]
-2022.0279474869753 [('Survived', 'Pclass'), ('Pclass', 'Sex')]
-2022.0143879516047 [('Pclass', 'Survived'), ('Pclass', 'Sex')]
-2021.9571134937144 [('Sex', 'Pclass'), ('Pclass', 'Survived')]
-2017.5258065853768 [('Survived', 'Pclass'), ('Sex', 'Pclass')]
-1941.3075053892835 [('Survived', 'Sex')]
-1941.2720031713893 [('Sex', 'Survived')]
-1937.4304608956886 [('Sex', 'Survived'), ('Pclass', 'Sex')]
-1937.4086886556925 [('Survived', 'Sex'), ('Sex', 'Pclass')]
-1937.3731864377983 [('Sex', 'Survived'), ('Sex', 'Pclass')]
-1934.134485060888 [('Survived', 'Sex'), ('Pclass', 'Sex')]
-1894.2637587114903 [('Survived', 'Sex'), ('Survived', 'Pclass')]
-1894.2501991761196 [('Survived', 'Sex'), ('Pclass', 'Survived')]
-1894.228256493596 [('Survived', 'Pclass'), ('Sex', 'Survived')]
-1891.0630673606006 [('Sex', 'Survived'), ('Pclass', 'Survived')]
-1887.2215250849 [('Sex', 'Survived'), ('Pclass', 'Survived'), ('Pclass', 'Sex')]
-1887.1642506270096 [('Sex', 'Survived'), ('Sex', 'Pclass'), ('Pclass', 'Survived')]
-1887.0907383830947 [('Survived', 'Sex'), ('Survived', 'Pclass'), ('Pclass', 'Sex')]
-1887.077178847724 [('Survived', 'Sex'), ('Pclass', 'Survived'), ('Pclass', 'Sex')]
-1885.9200755341908 [('Survived', 'Sex'), ('Survived', 'Pclass'), ('Sex', 'Pclass')]
-1885.8845733162966 [('Survived', 'Pclass'), ('Sex', 'Survived'), ('Sex', 'Pclass')]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here it didn’t work as I hoped. &lt;code&gt;[('Sex', 'Survived'), ('Pclass', 'Survived')]&lt;/code&gt; has the best score among models with 2 or less edges, but every model with 3 edges scores better. I didn’t have a closer look at the dataset yet, but a weak dependency between &lt;code&gt;Sex&lt;/code&gt; and &lt;code&gt;PClass&lt;/code&gt; would explain this.&lt;/p&gt;</summary></entry><entry><title>Score-based Structure Learning BNs</title><link href="http://pgmpy.chrisittner.de/2016/06/24/score-based-structure-learning-bns.html" rel="alternate"></link><published>2016-06-24T00:00:00+02:00</published><author><name>Chris Ittner</name></author><id>tag:pgmpy.chrisittner.de,2016-06-24:2016/06/24/score-based-structure-learning-bns.html</id><summary type="html">&lt;p&gt;With a bit of delay, I am now working on a basic PR for score-based structure estimation for Bayesian Networks.
It comes with two ingredients:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;StructureScore&lt;/code&gt;-Class and its subclasses &lt;code&gt;BayesianScore&lt;/code&gt; and &lt;code&gt;BICScore&lt;/code&gt;. They are initialized with a data set and provide a &lt;code&gt;score&lt;/code&gt;-method to compute how well a given &lt;code&gt;BayesianModel&lt;/code&gt; can be fitted to the data, according to different criteria. Since those scores are decomposable for BNs, a &lt;code&gt;local_score&lt;/code&gt;-method is also exposed for node-by-node computation.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;StructureSearch&lt;/code&gt;-Class and its subclasses &lt;code&gt;ExhaustiveSearch&lt;/code&gt; and &lt;code&gt;HCSearch&lt;/code&gt;. They are initialized with a &lt;code&gt;StructureScore&lt;/code&gt;-instance and optimize that score over all &lt;code&gt;BayesianModel&lt;/code&gt;s. The latter subclass has a number of 
optional search enhancements.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So far &lt;code&gt;BayesianScore&lt;/code&gt; supports BDeu and K2 priors, I’ll think for a good interface to specify other prior weights.
With K2 priors the score is given by the following form:&lt;/p&gt;
&lt;div class="math"&gt;$$score^{K2}_D(m) = \log(P(m)) + \sum_{X\in nodes(m)} local\_score^{K2}_D(X, parents_m(X))$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(P(m)\)&lt;/span&gt; is an optional structure prior that is quite negligible in practice. &lt;span class="math"&gt;\(local\_score^{K2}\)&lt;/span&gt; is computed for each node as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$local\_score^{K2}_D(X, P_X) = \sum_{j=1}^{q(P_X)} (\log(\frac{(r-1)!}{(N_j+r-1)!}) + \sum_{k=1}^r \log(N_{jk}!))$$&lt;/div&gt;
&lt;p&gt;Where &lt;span class="math"&gt;\(r\)&lt;/span&gt; is the cardinality of the variable &lt;span class="math"&gt;\(X\)&lt;/span&gt;, &lt;span class="math"&gt;\(q(P_X)\)&lt;/span&gt; is the product of the cardinalities of the parents of &lt;span class="math"&gt;\(X\)&lt;/span&gt; (= the possible states of &lt;span class="math"&gt;\(P_X\)&lt;/span&gt;) and &lt;span class="math"&gt;\(N_{jk}\)&lt;/span&gt; is the number of times that variable &lt;span class="math"&gt;\(X\)&lt;/span&gt; is in state &lt;span class="math"&gt;\(k\)&lt;/span&gt; while parents are in state &lt;span class="math"&gt;\(j\)&lt;/span&gt; in the data sample. Finally, &lt;span class="math"&gt;\(N_j:=\sum_{k=1}^r N_{jk}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;PR will follow shortly.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary></entry><entry><title>Bayesian Parameter Estimation for BNs</title><link href="http://pgmpy.chrisittner.de/2016/06/10/bayesian-parameter-estimation-for-bns.html" rel="alternate"></link><published>2016-06-10T00:00:00+02:00</published><author><name>Chris Ittner</name></author><id>tag:pgmpy.chrisittner.de,2016-06-10:2016/06/10/bayesian-parameter-estimation-for-bns.html</id><summary type="html">&lt;p&gt;Now that ML Parameter Estimation works well, I’ve turned to Bayesian Parameter Estimation (all for discrete variables).&lt;/p&gt;
&lt;p&gt;The Bayesian approach is, in practice, very similar to the ML case.
Both involves counting how often each state of the variable obtains in the data, conditional of the parents state.
I thus factored out &lt;code&gt;state_count&lt;/code&gt;-method and put it the &lt;code&gt;estimators.BaseEstimator&lt;/code&gt; class from which all estimators inherit.
MLE is basically done after taking and normalizing the state counts.
For Bayesian Estimation (with dirichlet priors), one additionally specifies so-called &lt;code&gt;pseudo_counts&lt;/code&gt; for each variable state that encode prior beliefs. The state counts are then added to these virtual counts before normalization. My next PR will implement &lt;code&gt;estimators.BayesianEstimator&lt;/code&gt; to compute the CPD parameters for a &lt;code&gt;BayesianModel&lt;/code&gt;, given data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: &lt;a href="https://github.com/pgmpy/pgmpy/pull/696"&gt;#696&lt;/a&gt; has been merged and also added basic support for missing data. Bayesian parameter estimation is now working:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pgmpy.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BayesianModel&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pgmpy.estimators&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BayesianEstimator&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'A'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;'C'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]})&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BayesianModel&lt;/span&gt;&lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="s1"&gt;'A'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'C'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'C'&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;estimator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BayesianEstimator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt;&amp;gt;&amp;gt; print(estimator.estimate_cpd('A', prior_type="dirichlet", pseudo_counts=[1, 1]))
╒══════╤═════╕
│ A(0) │ 0.6 │
├──────┼─────┤
│ A(1) │ 0.4 │
╘══════╧═════╛
&amp;gt;&amp;gt;&amp;gt; print(estimator.estimate_cpd('A', prior_type="dirichlet", pseudo_counts=[2, 2]))
╒══════╤══════════╕
│ A(0) │ 0.571429 │
├──────┼──────────┤
│ A(1) │ 0.428571 │
╘══════╧══════════╛
&amp;gt;&amp;gt;&amp;gt; print(estimator.estimate_cpd('A', prior_type="dirichlet", pseudo_counts=[5, 5]))
╒══════╤══════════╕
│ A(0) │ 0.538462 │
├──────┼──────────┤
│ A(1) │ 0.461538 │
╘══════╧══════════╛
&amp;gt;&amp;gt;&amp;gt; for cpd in estimator.get_parameters(prior_type="BDeu", equivalent_sample_size=10):
...     print(cpd)
...
╒══════╤══════════╕
│ A(0) │ 0.538462 │
├──────┼──────────┤
│ A(1) │ 0.461538 │
╘══════╧══════════╛
╒══════╤═════════════════════╤═════════════════════╤═════════════════════╤══════╕
│ A    │ A(0)                │ A(0)                │ A(1)                │ A(1) │
├──────┼─────────────────────┼─────────────────────┼─────────────────────┼──────┤
│ B    │ B(0)                │ B(1)                │ B(0)                │ B(1) │
├──────┼─────────────────────┼─────────────────────┼─────────────────────┼──────┤
│ C(0) │ 0.35714285714285715 │ 0.35714285714285715 │ 0.6428571428571429  │ 0.5  │
├──────┼─────────────────────┼─────────────────────┼─────────────────────┼──────┤
│ C(1) │ 0.6428571428571429  │ 0.6428571428571429  │ 0.35714285714285715 │ 0.5  │
╘══════╧═════════════════════╧═════════════════════╧═════════════════════╧══════╛
╒══════╤══════════╕
│ B(0) │ 0.538462 │
├──────┼──────────┤
│ B(1) │ 0.461538 │
╘══════╧══════════╛
&lt;/pre&gt;&lt;/div&gt;</summary></entry><entry><title>MLE Parameter Estimation for BNs</title><link href="http://pgmpy.chrisittner.de/2016/05/18/mle-parameter-estimation-for-bns.html" rel="alternate"></link><published>2016-05-18T00:00:00+02:00</published><author><name>Chris Ittner</name></author><id>tag:pgmpy.chrisittner.de,2016-05-18:2016/05/18/mle-parameter-estimation-for-bns.html</id><summary type="html">&lt;p&gt;At the moment pgmpy supports Maximum Likelihood Estimation (MLE) to estimate the conditional probability tables (CPTs) for the variables of a Bayesian Network, given some data set. In my first PR, I’ll refactor the current MLE parameter estimation code to make it a bit nicer to use.
This includes properly using pgmpy’s state name feature, removing the current limitation to &lt;code&gt;int&lt;/code&gt;-data and allowing to specify the states that each variable might take in advance, rather than reading it from the data. The latter will be necessary for Bayesian Parameter estimation, where non-occurring states get nonzero probabilities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; &lt;a href="https://github.com/pgmpy/pgmpy/pull/694"&gt;#694&lt;/a&gt; has been merged, and &lt;code&gt;MaximumLikelihoodEstimator&lt;/code&gt; now supports the above features, including non-numeric variables:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pgmpy.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BayesianModel&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pgmpy.estimators&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MaximumLikelihoodEstimator&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BayesianModel&lt;/span&gt;&lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="s1"&gt;'Light?'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Color'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Fruit'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Color'&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'Fruit'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Apple'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Apple'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Apple'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Banana'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Banana'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                          &lt;span class="s1"&gt;'Light?'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                          &lt;span class="s1"&gt;'Color'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'red'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="s1"&gt;'green'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'black'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'black'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="s1"&gt;'yellow'&lt;/span&gt;&lt;span class="p"&gt;]})&lt;/span&gt;
&lt;span class="n"&gt;mle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MaximumLikelihoodEstimator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_estimate_cpd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Color'&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;╒═══════════════╤═══════════════╤══════════════╤═══════════════╤═══════════════╕
│ Fruit         │ Fruit(Apple)  │ Fruit(Apple) │ Fruit(Banana) │ Fruit(Banana) │
├───────────────┼───────────────┼──────────────┼───────────────┼───────────────┤
│ Light?        │ Light?(False) │ Light?(True) │ Light?(False) │ Light?(True)  │
├───────────────┼───────────────┼──────────────┼───────────────┼───────────────┤
│ Color(black)  │ 1.0           │ 0.0          │ 1.0           │ 0.0           │
├───────────────┼───────────────┼──────────────┼───────────────┼───────────────┤
│ Color(green)  │ 0.0           │ 0.5          │ 0.0           │ 0.0           │
├───────────────┼───────────────┼──────────────┼───────────────┼───────────────┤
│ Color(red)    │ 0.0           │ 0.5          │ 0.0           │ 0.0           │
├───────────────┼───────────────┼──────────────┼───────────────┼───────────────┤
│ Color(yellow) │ 0.0           │ 0.0          │ 0.0           │ 1.0           │
╘═══════════════╧═══════════════╧══════════════╧═══════════════╧═══════════════╛
&lt;/pre&gt;&lt;/div&gt;</summary></entry><entry><title>GSoC proposal accepted!</title><link href="http://pgmpy.chrisittner.de/2016/04/26/first-post.html" rel="alternate"></link><published>2016-04-26T00:00:00+02:00</published><author><name>Chris Ittner</name></author><id>tag:pgmpy.chrisittner.de,2016-04-26:2016/04/26/first-post.html</id><summary type="html">&lt;p&gt;My proposal for &lt;a href="https://summerofcode.withgoogle.com/"&gt;Google Summer of Code 2016&lt;/a&gt; has 
been accepted :). This means that I will spend part of my summer working on the &lt;a href="http://pgmpy.org"&gt;pgmpy&lt;/a&gt; library. I will implement some techniques for Bayesian Network 
structure learning. You can have a look an my proposal 
&lt;a href="http://pgmpy.chrisittner.de/pages/gsoc-proposal.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As a first step, I set up this blog to document my progress. It is built with the &lt;a href="http://blog.getpelican.com/"&gt;Pelican&lt;/a&gt; static-site generator and hosted on &lt;a href="https://pages.github.com/"&gt;GitHub pages&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Updates will follow!&lt;/p&gt;</summary></entry></feed>